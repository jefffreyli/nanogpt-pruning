# WikiText-2

WikiText-2 (raw) language modeling dataset, prepared with GPT-2 BPE tokens for use with NanoGPT.

After running `prepare.py`:

- `train.bin` has 2,415,651 tokens
- `val.bin` has 249,750 tokens
